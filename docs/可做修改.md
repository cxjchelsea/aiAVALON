### 1. 把 “信念系统” 真正接到 LLM 上，而不是摆设

现在你有 `belief_system.py`，但要检查：

- 在生成发言 / 做决定时，是不是把**每个人的嫌疑度**、历史行为总结传给 LLM？
- 例：给 LLM 的上下文里加一段：

```text
当前你对其他玩家的怀疑度（0-100）：
- 玩家2：80（多次反对你认同的队伍）
- 玩家3：30（行为中立）
- 玩家4：10（大量支持你阵营成功任务）

你可以根据这些“怀疑度”来决定是否支持队伍、是否洗白/甩锅某人。
```

如果信念只在 Python 里更新，没喂回模型，那它等于没用。

------

### 2. 决策不要“一步到位”，而是拆成：思考 → 决策

现在你多半是：

```python
llm("你现在要选队伍/投票，给个结果和一句理由")
```

可以改成两步（对 LLM 可见的 chain-of-thought）：

1. 内部思考（不直接展示给其他玩家）：

   > - 当前公共信息是什么？
   > - 谁的行为可疑？
   > - 你这一步有哪些选项？各自利弊？
   > - 你最想达成的长期目标是什么？

2. 根据思考，输出结构化决策：

```json
{
  "action": "vote_reject",
  "public_reason": "我觉得2号行为太怪，他每次都在关键局点同意危险队伍。",
  "private_thought": "...（不广播）"
}
```

你可以在代码里只展示 `public_reason` 给其他 agent，`private_thought` 只进入 agent 的本地记忆。

**涌现感很大一部分来自这个“思考层”。**

------

### 3. 强行给 agent 植入“性格 + 风格”

现在角色只有“阵营+身份”。你可以增加：

- “偏理性 vs 偏冲动”
- “爱多说 vs 爱少说”
- “爱带节奏 vs 爱跟风”
- “高风险偏好 vs 稳健控”

在 prompt 里变成这样：

> 你是 3 号玩家，忠臣。
>  你的性格：
>
> - 非常多疑
> - 不容易被几句话说动
> - 喜欢自己带节奏，不喜欢别人压你
>    在发言和投票时，你要体现这一点。

LLM 有了“角色扮演空间”，行为会丰富很多。

------

### 4. 给它“对局级记忆”，而不是只看当前回合

你可以为每个 agent 维护一个简单的 “局记忆”：

- 谁在第几轮支持了你提的队伍
- 谁在任务失败时在队伍里
- 谁经常跳出来带节奏抹黑你

然后每轮调用 LLM 时，把一个 **小型摘要**喂进去，比如：

```text
对局回顾摘要（仅供你参考）：
- 第1轮：你提队[2,3,4]，2、3同意，4、5反对，任务成功。
- 第2轮：4提队[1,4,5]，你反对，最终任务失败。
你的初步判断：
- 4 号可信度：30/100（可能是坏人）
- 5 号可信度：40/100（待观察）
```

这样 LLM 才会“延续上一轮的怀疑”，而不是每轮都 reset。

------

### 5. 把“大规则”别写太死，给 LLM 决策空间

很多“没涌现”的根源，是系统 prompt 里写了类似：

> “你是忠臣，你必须始终站在好人一边，不能撒谎。”

那它永远不会出现“好人装坏人、为了骗刺客”的高级操作。

你可以改成更松的原则，比如：

> - 你是忠臣，目标是让好人赢。
> - 你**通常**会说真话，但如果你认为在某些局势下“混淆视听”能帮助好人获胜，你可以适度隐瞒或误导。
> - 不要无意义撒谎，重点是“为了阵营利益”。

坏人也一样，不要写死“你必须每次都投反对、每次都破坏任务”，那它只会变成明显坏人。

------

### 6. 给一点“探索性”——模型参数和策略多样化

- 不要所有人都用同一个温度 / 同一个策略 prompt。
- 有的 agent 温度 0.7，多样化一点；有的 0.3 更稳。
- 有的 agent 多考虑“阵营胜率”，有的更偏“自保”。

多样性是涌现的前提之一。

------

### 7. 记录对局日志，观察哪些地方看起来“太机械”

你可以做到：

- 每局输出一个 `game_log.json`：
  - 每轮队伍提名
  - 每个 agent 的 vote / 公开发言
  - 模型的 private_thought（仅用来分析，不在游戏中泄露）

然后你手动看几局，会很快发现：

- 哪些提示下行为很扁平
- 哪些节点没被用上（比如 belief 没进 prompt）
- 哪些地方可以加“博弈思考”

这比一直盯着代码感觉明显多了。

